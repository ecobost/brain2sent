# Written by: Erick Cobos T
# Date: 03-Aug-2016
"""
This is NOT a script.

It is a log of the different things I needed to test before training the big 
models. It is not polished as I was just collecting the different results and
checking what could work. This wouldn't work as is because I have renamed some
data files and deleted some others that I end up not using.


SUMMARY:
I tested some things for preprocessing data. I decided:
+ To average the image feature vectors over an entire second rather than just
	subsample them every 15 frames.
+ To use the image feature vectors as outputted by the network rather than
	accesing them before the ReLU (so they could have negative values).
+ To test with a smoothed version of the BOLD activations. Smoothed with a 
	gaussian filter (std = 1.5secs). I would also train with the normal BOLD.


I decided to go for the following models*:

1 Fit an l1-regularized regression (Lasso) on all input features (no voxel 
	selection). The regularization parameter (called alpha in scikit-learn) is
	selected via 5-fold cross-validation in a range from high to low sparsity 
	(100 alphas given by scikit-learn)
2 Fit an l2-regularized regression (Ridge) on all input features (no voxel
	selection). The regularization parameter (called alpha in scikit-learn) is
	selected via 5-fold cross-validation in the range {10^x | x in [3.5, 3.6,
	3.7, 3.8 ..., 5.2, 5.3, 5.4, 5.5]}
3 Select best features using an F test (f_regression) and fit a ridge regression.
	Number of features k in range {1, 3, 7, 10, 25, 50, 75, 100, 250, 500, 750,
	1000, 2500, 5000}. Regularization parameter in range {10^x | x in [1.5, 1.6,
	1.7, 1.8, ..., 4.2, 4.3, 4.4, 4.5]}
4 Use ROIs as feature selection. Fit a separate ridge regression for each ROI 
	and retain the one that gave the best CV. Regularization parameter 
	cross-validated as in 3.
5 Use the trained lasso model as feature selection. Select all features that 
	have a coefficient different than zero in the lasso model and fit a ridge
	regression. Regularization parameter cross-validated as in 3.
6 Use the trained ridge model as feature selection. Choose the k features with 
	highest absolute value and refit a ridge regression model. k and 
	regularization parameter cross-validated as in 3.
7 Fit a neural network with a single hidden layer with 400 units, output is the 
	768-d vector representing the image vectors. Trained with SGD+NAG, learning 
	rate is divided by 5 if the training loss has not decreased in two epochs, 
	training stops when learning rate is too small. Regularization parameter 
	(called alpha by scikit-learn) is searched using 5-fold cv in range {10^x | 
	x in {2, 2.05, 2.1, 2.15, 2.2, 2.25, 2.3, 2.35, 2.4, 2.45, 2.5, 2.55,
	2.6, 2.65, 2.7, 2.75, 2.8, 2.85, 2.9, 2.95, 3}}

* All models (except for the neural network) are fitted for each of the 768 
output units to predict.

Other things that could be tried:
+ Feature selection with ExtraTrees.
+ Other regression models: random Forest/ExtraTrees, SVR, Nearest Neighbor
+ Dimensionality reduction: PCA

See train_models.py for the scikit-learn code declaring and training each model.
"""
import numpy as np
import h5py
# Read input
Xs_file = h5py.File('train_bold.h5', 'r')
Xs = np.array(Xs_file['responses'])
Xs_file.close()

# Read regression targets
y_file = h5py.File('train_1sec_feats.h5', 'r')
y = np.array(y_file['feats'])
y_file.close()
 
# Delay it 5 secs and drop first and last 10 samples
delay = 5
Xs = Xs[10 + delay: -10 + delay, :]
y = y[10:-10, :]

# Get last 10% as test set. I se feature 3 for tests. Hopefully, is a good proxy.
X_train = Xs[:-718, :]; y_train = y[:-718, 3]; X_test = Xs[-718:,:]; y_test = y[-718:, 3]
# Baseline mse (predicting y_train.mean()) = 0.025472971





###############################################################################
# Testing selectK best
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn import linear_model

num_features = [1, 3, 10, 31, 100, 316, 1000, 3160]
ridge_alphas = [10, 31.6, 100, 316, 1000, 3160, 10000, 31600, 100000]
for k in num_features:
	print(k, 'feature(s)'); selector = SelectKBest(f_regression, k=k).fit(X_train, y_train); X_new = selector.transform(X_train); X_test_new = selector.transform(X_test); model = linear_model.RidgeCV(alphas=ridge_alphas, cv=5).fit(X_new, y_train); y_pred = model.predict(X_test_new); mse = ((y_test - y_pred)**2).mean(); print('Ridge: alpha =', model.alpha_, 'mse =', mse); model = linear_model.LassoCV(cv=5, n_jobs=-1, selection='random', random_state=123, eps = 0.008).fit(X_new, y_train); print('Min', model.alphas_.min(), 'Max', model.alphas_.max()); sparsity = (model.coef_ != 0).sum(); y_pred = model.predict(X_test_new); mse = ((y_test - y_pred)**2).mean(); print('Lasso: alpha=', model.alpha_, 'sparsity =', sparsity , 'mse =', mse)

# Results for selectKBest
1 feature(s)
Ridge: alpha = 316 mse = 0.0193451730488
Lasso: alpha= 0.000199906355795 sparsity = 1 mse = 0.0193563506232
3 feature(s)
Ridge: alpha = 316 mse = 0.0191307509213
Lasso: alpha= 0.000135325402801 sparsity = 3 mse = 0.0191628579329
10 feature(s)
Ridge: alpha = 1000 mse = 0.0186510786721
Lasso: alpha= 0.000613744360206 sparsity = 9 mse = 0.018682844637
31 feature(s)
Ridge: alpha = 1000 mse = 0.018971001032
Lasso: alpha= 0.000710444639961 sparsity = 23 mse = 0.0190695104827
100 feature(s)
Ridge: alpha = 3160 mse = 0.018676169905
Lasso: alpha= 0.00147653545676 sparsity = 50 mse = 0.0187523667855
316 feature(s)
Ridge: alpha = 3160 mse = 0.0188596154879
Lasso: alpha= 0.00155033227947 sparsity = 134 mse = 0.0188013253002
1000 feature(s)
Ridge: alpha = 10000 mse = 0.0187657339141
Lasso: alpha= 0.00188429296401 sparsity = 288 mse = 0.0193014498216
3160 feature(s)
Ridge: alpha = 10000 mse = 0.0187265410358
Lasso: alpha= 0.00179459939597 sparsity = 739 mse = 0.0191146389525
All features (no feature selection)
Ridge: alpha = 31600 mse = 0.0182215347193
Lasso: alpha= 0.00347872947997 sparsity = 343 mse = 0.0185642872093

# Ridge is always better (and the regularization term could be better selected via cv)

# For f_regression maybe select anything with p < 0.01 (instead of going for different number of features) or add this to the option of number of features. something like [1, 2, 3, 5, 10 , 25, 70, 500, 1000, (selector.pvalues_ < 0.01).sum(), (selector.pvalues_ < 0.001).sum()] Make sure these are not zero, otherwise the whole thing will crash.

# Test using the LOOCV rather than 5 cv for ridge


# Test another y_i







###############################################################################
# Testing with normal features (not averaged over 1sec)

# Read regression targets
y2_file = h5py.File('train_feats.h5', 'r')
y2 = np.array(y2_file['feats'])
y2_file.close()

y2 = y2[10:-10, :]
y2_train = y2[:-718, 3]
y2_test = y2[-718:, 3]

# How different they are?
Subsampled (y2_train, y2_train.std())
mean = 0.1431753, std = 0.152648
1sec (y_train)
mean = 0.14328392, std = 0.13918754
# Same mean but slightly less variance. Makes sense
corr = 0.85578866 # So not exactly equal
(y_train == 0).sum() = 918
(y2_train == 0).sum() = 1968
# Quite more zeros in the subsampled version (again, as expected)

# In plotting they look pretty similar except that the 1sec version is less exagerated (peaks are not as high), it has less zero, because in the 15 frames it was averaged at least one probably had other different than zero. Otherwise, kind of the same, I like the 1sec version better, it is probably truer to reality.

# Are results much different?
num_features = [1, 3, 10, 31, 100, 316, 1000, 3160]
ridge_alphas = [10, 31.6, 100, 316, 1000, 3160, 10000, 31600, 100000]
for k in num_features:
	print(k, 'feature(s)'); selector = SelectKBest(f_regression, k=k).fit(X_train, y2_train); X_new = selector.transform(X_train); X_test_new = selector.transform(X_test); model = linear_model.RidgeCV(alphas=ridge_alphas, cv=5).fit(X_new, y2_train); y2_pred = model.predict(X_test_new); mse = ((y2_test - y2_pred)**2).mean(); print('Ridge: alpha =', model.alpha_, 'mse =', mse); model = linear_model.LassoCV(cv=5, n_jobs=-1, selection='random', random_state=123, eps = 0.008).fit(X_new, y2_train); print('Min', model.alphas_.min(), 'Max', model.alphas_.max()); sparsity = (model.coef_ != 0).sum(); y2_pred = model.predict(X_test_new); mse = ((y2_test - y2_pred)**2).mean(); print('Lasso: alpha=', model.alpha_, 'sparsity =', sparsity , 'mse =', mse)

1 feature(s)
Ridge: alpha = 100 mse = 0.0252800283064
Lasso: alpha= 0.00012447515569 sparsity = 1 mse = 0.0252803612046
3 feature(s)
Ridge: alpha = 316 mse = 0.0252221846815
Lasso: alpha= 0.000130696388614 sparsity = 3 mse = 0.0252448330584
10 feature(s)
Ridge: alpha = 1000 mse = 0.0251653823199
Lasso: alpha= 0.000363967563206 sparsity = 10 mse = 0.025274593093
31 feature(s)
Ridge: alpha = 1000 mse = 0.0250013556365
Lasso: alpha= 0.000686142784955 sparsity = 28 mse = 0.0250986211279
100 feature(s)
Ridge: alpha = 3160 mse = 0.0245972044289
Lasso: alpha= 0.0013581484116 sparsity = 57 mse = 0.0248419695539
316 feature(s)
Ridge: alpha = 3160 mse = 0.0245867338374
Lasso: alpha= 0.00181983781603 sparsity = 132 mse = 0.0246929585683
1000 feature(s)
Ridge: alpha = 10000 mse = 0.0243015366922
Lasso: alpha= 0.00200629367415 sparsity = 304 mse = 0.0248499015724
3160 feature(s)
Ridge: alpha = 10000 mse = 0.0243754628386
Lasso: alpha= 0.00200629367415 sparsity = 734 mse = 0.0251553122045
# This seems wrong, if they are so similar why would the fit be so bad.
# Plus y_pred and y2_pred are quite similar (mean diff is 0.0012) so it is pretty much learning the same.
# There is nothing wrong. It is learning around the same it is just that the subsampled version is quite noisy so the y_test will expect noisy answers, for instance the subsampled version has peaks or valleys (that last a single second) where the averaged has not meaning that just in that second there was something totallly different going on (somethiong like dog in second 0-3, no dog in second 4, dog in second 4-7) but as the video is continuos this doesn't quite add up, maybe it just happen that that frame had something different and the subsampled version catched that. Yep, averag over 1 sec is quite better (takes alittle noise 0out of the already quite noisy data.

del y2, y2_train y2_test, y2_pred 






###############################################################################
# Testing with features recorded before the RELU in the convnet. This will have both positive and negative values (not only positive) as the normal ones.


# Read regression targets
y3_file = h5py.File('train_feats_wo_relu.h5', 'r') # No relu/averaged over 1 sec
y3 = np.array(y3_file['feats'])
y3_file.close()

y3 = y3[10:-10, :]
y3_train = y3[:-718, 3]
y3_test = y3[-718:, 3] # y_test is equal to this but with all negatives send to zero.

num_features = [1, 3, 10, 31, 100, 316, 1000, 3160]
ridge_alphas = [10, 31.6, 100, 316, 1000, 3160, 10000, 31600, 100000]
for k in num_features:
	print(k, 'feature(s)'); selector = SelectKBest(f_regression, k=k).fit(X_train, y3_train); X_new = selector.transform(X_train); X_test_new = selector.transform(X_test);
	model = linear_model.RidgeCV(alphas=ridge_alphas, cv=5).fit(X_new, y3_train); y3_pred = model.predict(X_test_new); mse = ((y3_test - y3_pred)**2).mean(); y3_pred[y3_pred < 0] = 0; mse2 = ((y_test - y3_pred)**2).mean(); print('Ridge: alpha =', model.alpha_, 'mse =', mse, 'mse2 =', mse2);
	model = linear_model.LassoCV(cv=5, n_jobs=-1, selection='random', random_state=123, eps = 0.008).fit(X_new, y3_train); print('Min', model.alphas_.min(), 'Max', model.alphas_.max()); sparsity = (model.coef_ != 0).sum(); y3_pred = model.predict(X_test_new); mse = ((y3_test - y3_pred)**2).mean(); y3_pred[y3_pred < 0] = 0; mse2 = ((y_test - y3_pred)**2).mean(); print('Lasso: alpha=', model.alpha_, 'sparsity =', sparsity , 'mse =', mse, 'mse2 =', mse2)

1 feature(s)
Ridge: alpha = 316 mse = 0.0349971975841 mse2 = 0.0221583522705
Lasso: alpha= 0.000220577585725 sparsity = 1 mse = 0.0350238681169 mse2 = 0.0221953100882
3 feature(s)
Ridge: alpha = 1000 mse = 0.0346210062056 mse2 = 0.022058214069
Lasso: alpha= 0.000436610431905 sparsity = 3 mse = 0.0346588781308 mse2 = 0.0221336333489
10 feature(s)
Ridge: alpha = 316 mse = 0.0340131825871 mse2 = 0.022098772828
Lasso: alpha= 0.000396033833475 sparsity = 9 mse = 0.0340428127682 mse2 = 0.0221314536461
31 feature(s)
Ridge: alpha = 1000 mse = 0.0336698749675 mse2 = 0.0218266031307
Lasso: alpha= 0.00218305215952 sparsity = 15 mse = 0.033885423004 mse2 = 0.0219153649749
100 feature(s)
Ridge: alpha = 3160 mse = 0.0333453833768 mse2 = 0.0219829000697
Lasso: alpha= 0.00292515960725 sparsity = 31 mse = 0.0334655016971 mse2 = 0.0221098961792
316 feature(s)
Ridge: alpha = 3160 mse = 0.032752771119 mse2 = 0.0222750168244
Lasso: alpha= 0.00322486386656 sparsity = 94 mse = 0.0324801270661 mse2 = 0.0218454085681
1000 feature(s)
Ridge: alpha = 10000 mse = 0.0319847950218 mse2 = 0.0218010486521
Lasso: alpha= 0.00240672201634 sparsity = 294 mse = 0.0319544909149 mse2 = 0.0220760970334
3160 feature(s)
Ridge: alpha = 10000 mse = 0.0320022033308 mse2 = 0.0219051928925
Lasso: alpha= 0.00252700929906 sparsity = 725 mse = 0.0322022164097 mse2 = 0.0223584844
# So it is worse, why?. I thought having the negative values will encourage it 
# to learn more. Maybe now it is trying to fit the negative parts, too and then 
# when that goes to zero, it loses all that work.
# I still kind of thing that if I had more data it would probably be better to
# use this features.

# I also tested correlation with y_test (k=100)
0.20099122 for y_pred (computed with y_train)
0.17280401 for y3_pred (computed with y3_train and sett everything negative to zero)
0.20258783 for y_pred after setting the lowest 100 to zero (simulating the same level of sparsity as y_train)
0.17232713 for y3_pred after setting the lowest 100 to zero.

# Yep, it's worse. Stick with ReLUed ones

del y3, y3_train y3_test, y3_pred 





###############################################################################
# Testing setting a number of outputs to zero trying to simulate the sparsity 
# of the desired output (using as proxy the sparsity of y_train). Or changing the
# spread so it has the same standard deviation as the expected output (again using
# as proxy the y_train)

k=100
print(k, 'feature(s)')
selector = SelectKBest(f_regression, k=k).fit(X_train, y_train)
X_new = selector.transform(X_train)
X_test_new = selector.transform(X_test)
model = linear_model.RidgeCV(alphas=ridge_alphas, cv=5).fit(X_new, y_train)
y_pred = model.predict(X_test_new)
y_p = y_pred.copy()
# y_train has 0.142 percent of values equal to zero. That is equal to 102 in y_test
# (y_pred < 0.0991).sum() = 102, so
y_p[y_pred < 0.0991] = 0

# Corr for y_pred is 0.20099122 (mse 0.01867)
# corr for y_p is 0.19958189  (mse 0.02002)
# however if i set 101 (instead of 102) elemtns to zero (y_pred < 0.1), corr is 0.20258783, so it can be improved if the number is right.

# What about also improving the spread?
# All predictions have small std (spread), so if i increase the spread by subtracting the y_train.mean() multiplying it by some number and adding the y_train.mean() again (so that all values below the y_mean go lower and all above go higher), I'll probably get better results

# Or better use the predictions on the train set as a proxy of the difference between the std of the true y_pred (y_train)
y_train_pred = model.predict(X_new)
y_p = y_pred.copy()
expected_mean = y_train_pred.mean()
expected_std = y_train_pred.std()
desired_mean = y_train.mean() 
desired_std = y_train.std()
# the expected mean (0.143283) and desired mean(0.143283) are quite similar, i.e., 
# the model learns the mean of the outputs (usually in the bias/intercept term).
# the expected std (0.04207434) and desired_std (0.139187) though are different
# showing that the model is conservative and predicts things closer to the mean.

# Normalize
y_p = (y_p - expected_mean)/expected_std
# Give new spread
y_p = (y_p * desired_std) + desired_mean

# Plots look fine, it does now have kind of the same spread as the y_test
np.corrcoef(y_p, y_test) = 0.2009912
# Correlation didn't change as all we did was scale it (makes sense). Any linear
# transformation will still produce the same correlation. Mse however, should
# improve as now the features are closer in the 718-d space
((y_pred - y_test)**2).mean() = 0.018676169904963687
((y_p - y_test)**2).mean() = 0.028894233007613852 
# Nop :) I guess it is because the predictions are not as good

# But now, it predict negative numbers which I can set to zero as
y_p [y_p < 0] = 0
np.corrcoef(y_p, y_test) = 0.21247009 # goes up :)
((y_p - y_test)**2).mean() = 0.025909022 # goes down

# And if I try to induce the same sparsity as I did before
y_p[y_pred < 0.0991] = 0
np.corrcoef(y_p, y_test) = 0.21247009
((y_p - y_test)**2).mean() = 0.025909022 
# In fact, nothing changes here because setting to zero above already set the 
# lowest 104 to zero. I like that.

# So I can slightly improve correlation but mse goes down quite a bit (considering 0.0254 is thye baseline).
# Maybe that will change resulkts????

# Plus as the output of this is to be taken by the RNN maybe it is better to 
# have it go in the same spread because that is teh scale the RNN is expecting.

# Anyway, this is post-processing so it is probably better to present results 
# both wo and with this postprocessing. And choose then.

# It is quite important that I don't use any statistic extracted from the test set,
# not from test set features or test_set real labels or test_set predicted labels,
# everything could be done even if i had a single test set point. All is extracted
# from the training set.

# Plotting this against y_test looks quite better than plotting y_pred, that single
# 0.01 change in correlation is quite visible.






###############################################################################
# Testing smoothing the BOLD ativations temporally.
from scipy import ndimage
k=100
for std in [1, 2, 3, 5, 8, 10, 20]:
	print('Smoothed. std=', std); X_smooth = ndimage.gaussian_filter1d(X_train, std, 0); X_smooth = (X_smooth - X_smooth.mean(axis=0))/X_smooth.std(axis=0); X_test_smooth = ndimage.gaussian_filter1d(X_test, std, 0); X_test_smooth = (X_test_smooth - X_smooth.mean(axis=0))/X_smooth.std(axis=0)

	selector = SelectKBest(f_regression, k=k).fit(X_smooth, y_train); X_new = selector.transform(X_smooth); X_test_new = selector.transform(X_test_smooth);

	model = linear_model.RidgeCV(alphas=ridge_alphas, cv=5).fit(X_new, y_train); y_pred = model.predict(X_test_new); mse = ((y_test - y_pred)**2).mean(); print('Ridge: alpha =', model.alpha_, 'mse =', mse)

	model = linear_model.LassoCV(cv=5, n_jobs=-1, selection='random', random_state=123, eps = 0.008).fit(X_new, y_train); print('Max', model.alphas_.max(), 'Min', model.alphas_.min()); sparsity = (model.coef_ != 0).sum(); y_pred = model.predict(X_test_new); mse = ((y_test - y_pred)**2).mean(); print('Lasso: alpha=', model.alpha_, 'sparsity =', sparsity , 'mse =', mse)

# Tested by selecting the best 100 features with f_regression (separate for each model)
Normal 
Ridge: alpha = 3160 mse = 0.018676169905
Lasso: alpha= 0.00147653545676 sparsity = 50 mse = 0.0187523667855
Smoothed. std= 1
Ridge: alpha = 3160 mse = 0.0183967540448
Lasso: alpha= 0.00144694883455 sparsity = 64 mse = 0.0183892257329
Smoothed. std= 2
Ridge: alpha = 3160 mse = 0.0186137775244
Lasso: alpha= 0.00167033041265 sparsity = 59 mse = 0.0186122930673
Smoothed. std= 3
Ridge: alpha = 3160 mse = 0.0186012957738
Lasso: alpha= 0.00250148000215 sparsity = 53 mse = 0.0184625232682
Smoothed. std= 5
Ridge: alpha = 3160 mse = 0.0184542335409
Lasso: alpha= 0.00232399977777 sparsity = 48 mse = 0.0184104627988
Smoothed. std= 8
Ridge: alpha = 3160 mse = 0.0186771524289
Lasso: alpha= 0.00211054250093 sparsity = 54 mse = 0.0186959443378
Smoothed. std= 10
Ridge: alpha = 3160 mse = 0.0188072426215
Lasso: alpha= 0.00218829393143 sparsity = 51 mse = 0.0188154946324
Smoothed. std= 20
Ridge: alpha = 10000 mse = 0.0191955361617
Lasso: alpha= 0.00325127354768 sparsity = 32 mse = 0.0192277241503
# Consistently better. May be because it is bringing knowledge from previous/future
# timesteps instead of using a single delay (so it kind of assumes a gaussian hrf).
# An odd thing is that lasso in this case is consistently better than Ridge.
# Also it is good for std=1, bad for std=2,3, relatively good againg for std=5 and quite bad agin for std=8 and up

# Imma try it with 300 features but for another y_i (to select a std)
y4_train = y[:-718, 5]
y4_test = y[-718:, 5]
k=300

for std in [1, 2, 3, 5, 8, 10, 20]:
	print('Smoothed. std=', std); X_smooth = ndimage.gaussian_filter1d(X_train, std, 0); X_smooth = (X_smooth - X_smooth.mean(axis=0))/X_smooth.std(axis=0); X_test_smooth = ndimage.gaussian_filter1d(X_test, std, 0); X_test_smooth = (X_test_smooth - X_smooth.mean(axis=0))/X_smooth.std(axis=0)

	selector = SelectKBest(f_regression, k=k).fit(X_smooth, y4_train); X_new = selector.transform(X_smooth); X_test_new = selector.transform(X_test_smooth);

	model = linear_model.RidgeCV(alphas=ridge_alphas, cv=5).fit(X_new, y4_train); y4_pred = model.predict(X_test_new); mse = ((y4_test - y4_pred)**2).mean(); print('Ridge: alpha =', model.alpha_, 'mse =', mse)

	model = linear_model.LassoCV(cv=5, n_jobs=-1, selection='random', random_state=123, eps = 0.008).fit(X_new, y4_train); print('Max', model.alphas_.max(), 'Min', model.alphas_.min()); sparsity = (model.coef_ != 0).sum(); y4_pred = model.predict(X_test_new); mse = ((y4_test - y4_pred)**2).mean(); print('Lasso: alpha=', model.alpha_, 'sparsity =', sparsity , 'mse =', mse)

Normal
Ridge: alpha = 3160 mse = 0.0301175581331
Lasso: alpha= 0.00212280942724 sparsity = 102 mse = 0.0303893116312
Smoothed. std= 1
Ridge: alpha = 3160 mse = 0.0295984696287
Lasso: alpha= 0.00396326162263 sparsity = 59 mse = 0.030286748991
Smoothed. std= 2
Ridge: alpha = 3160 mse = 0.0292038162638
Lasso: alpha= 0.0038658493776 sparsity = 61 mse = 0.0296605678201
Smoothed. std= 3
Ridge: alpha = 3160 mse = 0.029276691546
Lasso: alpha= 0.00405214938716 sparsity = 63 mse = 0.0294840829659
Smoothed. std= 5
Ridge: alpha = 3160 mse = 0.0302324850159
Lasso: alpha= 0.00352369211508 sparsity = 63 mse = 0.0303588724737
Smoothed. std= 8
Ridge: alpha = 10000 mse = 0.0321759556154
Lasso: alpha= 0.00734807520086 sparsity = 27 mse = 0.032503593896
Smoothed. std= 10
Ridge: alpha = 3160 mse = 0.0324502876996
Lasso: alpha= 0.00679160293294 sparsity = 32 mse = 0.0330574465054
Smoothed. std= 20
Ridge: alpha = 10000 mse = 0.0332863839995 
Lasso: alpha= 0.00683864138666 sparsity = 39 mse = 0.0334058385649 # didn't converge
# Lasso is worse now. Results are consistently better again and plateau at std=2 (kinda makes sense). std=1 is not as good as in the other examples


# Let's test it with yet another one
y4_train = y[:-718, 100]
y4_test = y[-718:, 100]
k=300

Normal
Ridge: alpha = 3160 mse = 0.0405022062561
Lasso: alpha= 0.00248417791101 sparsity = 147 mse = 0.0401582021196
Smoothed. std= 1
Ridge: alpha = 10000 mse = 0.0400178433661
Lasso: alpha= 0.00437446573606 sparsity = 102 mse = 0.0400674398656
Smoothed. std= 2
Ridge: alpha = 10000 mse = 0.0401437964831
Lasso: alpha= 0.00474687630362 sparsity = 91 mse = 0.0400570101381
Smoothed. std= 3
Ridge: alpha = 10000 mse = 0.0405363662789
Lasso: alpha= 0.00515060647608 sparsity = 88 mse = 0.0406556341862
Smoothed. std= 5
Ridge: alpha = 10000 mse = 0.0413890243855
Lasso: alpha= 0.00539554344253 sparsity = 75 mse = 0.0413921618115
Smoothed. std= 8
Ridge: alpha = 31600 mse = 0.042618120245
Lasso: alpha= 0.00546075903161 sparsity = 69 mse = 0.042278015445
Smoothed. std= 10
Ridge: alpha = 31600 mse = 0.0429490661876
Lasso: alpha= 0.00531723367775 sparsity = 66 mse = 0.0427328297172
Smoothed. std= 20
Ridge: alpha = 31600 mse = 0.0437572606873
Lasso: alpha= 0.00724604415293 sparsity = 41 mse = 0.0438210841314 # didn't converge
# std=1 is better than others std (2 is still acceptable).

# Not sure whether 1 or 2 is a better std. I'll test another one.
y4_train = y[:-718, 300]
y4_test = y[-718:, 300]
k=300

Normal
Ridge: alpha = 3160 mse = 0.0183645834642
Lasso: alpha= 0.0017072460303 sparsity = 112 mse = 0.0184297697829
Smoothed. std= 1
Ridge: alpha = 3160 mse = 0.0181444501117
Lasso: alpha= 0.00235231066948 sparsity = 92 mse = 0.0179438976407
Smoothed. std= 2
Ridge: alpha = 3160 mse = 0.0180515494809
Lasso: alpha= 0.00244942214296 sparsity = 85 mse = 0.0178005409809
Smoothed. std= 3
Ridge: alpha = 3160 mse = 0.0180642031221
Lasso: alpha= 0.0027684640458 sparsity = 75 mse = 0.0179841793608
Smoothed. std= 5
Ridge: alpha = 10000 mse = 0.0183017200076
Lasso: alpha= 0.00344006552382 sparsity = 59 mse = 0.018045466841
Smoothed. std= 8
Ridge: alpha = 10000 mse = 0.0182646761465
Max 0.0148579645425 Min 0.00011886371634
Lasso: alpha= 0.00312011227795 sparsity = 51 mse = 0.0181561594239
Smoothed. std= 10
Ridge: alpha = 10000 mse = 0.0184655579629
Lasso: alpha= 0.00351045191188 sparsity = 44 mse = 0.0184454376779 #didn't converge
Smoothed. std= 20
Ridge: alpha = 10000 mse = 0.0186560970461
Lasso: alpha= 0.00277989524867 sparsity = 46 mse = 0.0187501623562 #didn't converge
# std = 2 is better again. Results are better overall. Maybe stick with 1.5


# Test the smoothed(std=1.5) with diff features in the y_train
num_features = [1, 3, 10, 31, 100, 316, 1000, 3160]
std=1.5
X_smooth = ndimage.gaussian_filter1d(X_train, std, 0); X_smooth = (X_smooth - X_smooth.mean(axis=0))/X_smooth.std(axis=0); X_test_smooth = ndimage.gaussian_filter1d(X_test, std, 0); X_test_smooth = (X_test_smooth - X_smooth.mean(axis=0))/X_smooth.std(axis=0)
for k in num_features:
	print(k, 'feature(s)'); selector = SelectKBest(f_regression, k=k).fit(X_smooth, y_train); X_new = selector.transform(X_smooth); X_test_new = selector.transform(X_test_smooth);
	model = linear_model.RidgeCV(alphas=ridge_alphas, cv=5).fit(X_new, y_train); y_pred = model.predict(X_test_new); mse = ((y_test - y_pred)**2).mean(); print('Ridge: alpha =', model.alpha_, 'mse =', mse); 
	model = linear_model.LassoCV(cv=5, n_jobs=-1, selection='random', random_state=123, eps = 0.008).fit(X_new, y_train); print('Min', model.alphas_.min(), 'Max', model.alphas_.max()); sparsity = (model.coef_ != 0).sum(); y_pred = model.predict(X_test_new); mse = ((y_test - y_pred)**2).mean(); print('Lasso: alpha=', model.alpha_, 'sparsity =', sparsity , 'mse =', mse)


1 feature(s)
Ridge: alpha = 316 mse = 0.0191694913008
Min 0.000168149298776 Max 0.021018662347
Lasso: alpha= 0.000301901986874 sparsity = 1 mse = 0.019170954979
3 feature(s)
Ridge: alpha = 1000 mse = 0.0190520887221
Min 0.000168149298776 Max 0.021018662347
Lasso: alpha= 0.000800726415105 sparsity = 3 mse = 0.0190523746713
10 feature(s)
Ridge: alpha = 1000 mse = 0.0190371239615
Lasso: alpha= 0.00112655012529 sparsity = 10 mse = 0.019068242443
31 feature(s)
Ridge: alpha = 1000 mse = 0.0184393879801
Lasso: alpha= 0.00102185365287 sparsity = 25 mse = 0.0184857604736
100 feature(s)
Ridge: alpha = 3160 mse = 0.0185386284349
Lasso: alpha= 0.0013692229097 sparsity = 61 mse = 0.0186458330232
316 feature(s)
Ridge: alpha = 3160 mse = 0.0187167519323
Lasso: alpha= 0.00222988902136 sparsity = 103 mse = 0.0185475658638
1000 feature(s)
Ridge: alpha = 10000 mse = 0.0181268651463
Lasso: alpha= 0.00245835766144 sparsity = 192 mse = 0.0184483895772
3160 feature(s)
Ridge: alpha = 10000 mse = 0.0176693582849
Lasso: alpha= 0.00245835766144 sparsity = 365 mse = 0.0184727807949
# That was unexpectedly good. Plus, there is a gradiet of being better for more
# features (and alpha gets high, too but still not the highest)
# Also, Lasso takes a lot of time and is quite worse.

# Also tested it for 5000 features as above (Ridge: alpha = 10000 mse = 0.0176105107288) and for std=5 (3160 feature(s): Ridge: alpha = 31600 mse = 0.018474939248). 5000 is slightly better.


# Conclusion: Do run experiments with the smoothed version (std=1.5) and non-smoothed. And also do run for 5000 features and only ridge.

del y4, y4_train y4_test, y4_pred 







###############################################################################
# Timing how much it would take ridge and lasso on all features for feature y_3
import time

model = linear_model.RidgeCV(alphas=ridge_alphas, cv=5);
t1 = time.time()
model.fit(X_train, y_train)
t_ridge_cv = time.time() - t1
print(t_ridge_cv)
y_pred = model.predict(X_test)
mse = ((y_test - y_pred)**2).mean()
print('Ridge: alpha =', model.alpha_, 'mse =', mse)
# t = 133.06 (2.21 min)
# Ridge: alpha = 31600 mse = 0.0182215347193
# Nice, doesn't take much and produces good results witouth much hassle.
# there was an alpah=10000 so it didn't even use the highest alpha.


model = linear_model.LassoCV(cv=5, n_jobs=-1, selection='random', random_state=123, eps = 0.01)
t1 = time.time()
model.fit(X_train, y_train)
t_lasso_cv = time.time() - t1
print(t_lasso_cv)
print('Max', model.alphas_.max(), 'Min', model.alphas_.min())
sparsity = (model.coef_ != 0).sum()
y_pred = model.predict(X_test)
mse = ((y_test - y_pred)**2).mean()
print('Lasso: alpha=', model.alpha_, 'sparsity =', sparsity , 'mse =', mse)
# t = 1189.38 (19.82 min)
# Lasso: alpha= 0.00347872947997 sparsity = 343 mse = 0.0185642872093
# would take 10 days and results aren't as good. I was expecting this to be better



























###############################################################################
# Test the ROI version thing
# Read ROI assignments
roi_file = h5py.File('roi_info.h5', 'r')
roi = np.array(roi_file['rois'])
roi_file.close()

# Get masks
masks = {}
masks['v1'] = (roi == 19) | (roi == 20)
masks['v2'] = (roi == 21) | (roi == 22)
masks['v3'] = (roi == 27) | (roi == 28)
masks['v3a'] = (roi == 23) | (roi == 24)
masks['v3b'] = (roi == 25) | (roi == 26)
masks['v4'] = (roi == 29) | (roi == 30)
masks['lo'] = (roi == 17) | (roi == 18)
masks['mt'] = (roi == 5) | (roi == 6) | (roi == 7) | (roi == 8) #v5
masks['ip'] = (roi == 3) | (roi == 4)
masks['vo'] = (roi == 15) | (roi == 16)
masks['OBJ'] = (roi == 9) | (roi == 10) #?
masks['rsc'] = (roi == 13)
masks['sts'] = (roi == 14)
for ROI_name, mask in masks.items():
	print(ROI_name,':', mask.sum(), 'features'); X_new = X_train[:, mask]; X_test_new = X_test[:, mask];

	model = linear_model.RidgeCV(alphas=ridge_alphas, cv=5).fit(X_new, y_train); y_pred = model.predict(X_test_new); mse = ((y_test - y_pred)**2).mean(); print('Ridge: alpha =', model.alpha_, 'mse =', mse)

	model = linear_model.LassoCV(cv=5, n_jobs=-1, selection='random', random_state=123, eps = 0.01).fit(X_new, y_train); print('Max', model.alphas_.max(), 'Min', model.alphas_.min()); sparsity = (model.coef_ != 0).sum(); y_pred = model.predict(X_test_new); mse = ((y_test - y_pred)**2).mean(); print('Lasso: alpha=', model.alpha_, 'sparsity =', sparsity , 'mse =', mse)
	
	
	
	

###############################################################################
# Fit an ordinary linear model to see whether it overfits. Yep, it does.
model = linear_model.LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test_new)
mse = ((y_test - y_pred)**2).mean()
print('Linear Regression: mse =', mse)

Linear Regression: mse = 1.52775774527
# Wildly overfits. Takes a long time, too.

# Use the weights as feature selection
sorted_indices = abs(model.coef_).argsort()

for k in num_features:
	print(k, 'feature(s)'); selected_features = sorted_indices[-k:]; X_new = X_train[:, selected_features]; X_test_new = X_test[:, selected_features];
	model = linear_model.RidgeCV(alphas=ridge_alphas, cv=5).fit(X_new, y_train); y_pred = model.predict(X_test_new); mse = ((y_test - y_pred)**2).mean(); print('Ridge: alpha =', model.alpha_, 'mse =', mse); model = linear_model.LassoCV(cv=5, n_jobs=-1, selection='random', random_state=123, eps = 0.008).fit(X_new, y_train); print('Min', model.alphas_.min(), 'Max', model.alphas_.max()); sparsity = (model.coef_ != 0).sum(); y_pred = model.predict(X_test_new); mse = ((y_test - y_pred)**2).mean(); print('Lasso: alpha=', model.alpha_, 'sparsity =', sparsity , 'mse =', mse)
	
1 feature(s)
Ridge: alpha = 3160 mse = 0.0193359925193
Lasso: alpha= 2.422115595e-05 sparsity = 1 mse = 0.0193451448598
3 feature(s)
Ridge: alpha = 1000 mse = 0.0193654270057
Lasso: alpha= 4.52400746928e-05 sparsity = 3 mse = 0.0193803659414
10 feature(s)
Ridge: alpha = 10000 mse = 0.019359608142
Lasso: alpha= 0.0056550093366 sparsity = 1 mse = 0.0193221658192
31 feature(s)
Ridge: alpha = 3160 mse = 0.0189222740885
Lasso: alpha= 0.00216765666897 sparsity = 15 mse = 0.0188953893778
100 feature(s)
Ridge: alpha = 10000 mse = 0.0191179935357
Lasso: alpha= 0.00210924042808 sparsity = 47 mse = 0.0190391119692
316 feature(s)
Ridge: alpha = 10000 mse = 0.0194310242953
Lasso: alpha= 0.00355612917641 sparsity = 51 mse = 0.0191132484336
1000 feature(s)
Ridge: alpha = 10000 mse = 0.0189461914724
Lasso: alpha= 0.00262245457614 sparsity = 187 mse = 0.0193127331903
3160 feature(s)
Ridge: alpha = 10000 mse = 0.0187013939087
Lasso: alpha= 0.00289114446382 sparsity = 339 mse = 0.019056575336
# Results are somehow good because the Ridge, Lasso can still learn something 
# even if feats are somehow random. Still, f_regression is better.





###############################################################################
# Test a neural network and see what kind of regularization should it use
# Baseline correlation: 0.526314
from sklearn.neural_network import MLPRegressor

hidden_layer_size = [50, 100, 300, 600, 1000, 1500, 2000]
regularization_params = [10, 21.5, 46.3, 100, 215, 463, 1000, 2150, 4630, 10000, 21500]
y_train = y[:-718]; y_test =y[-718:]

for layer_size in hidden_layer_size:
	print('Layer size:', layer_size)
	for alpha in regularization_params:
		print('alpha = ', alpha); model = MLPRegressor(hidden_layer_sizes = (layer_size,), alpha=alpha, max_iter=1000, random_state=123, early_stopping=True, tol = 1e-8); model.fit(X_train, y_train); y_pred = model.predict(X_test); mse = ((y_pred-y_test)**2).mean(); mse3 = ((y_pred[:,3] - y_test[:,3])**2).mean(); corr = 0; corrf = 0;
		for i in range(718):
			corr +=  np.corrcoef(y_pred[i,:], y_test[i,:])[0,1]
		for i in range(768):
			corrf +=  np.corrcoef(y_pred[:,i], y_test[:,i])[0,1]
		corr =	corr/718; corrf =	corrf/768; print('mse =', mse, 'mse3 =',mse3, 'corr =', corr, 'corr/f =', corrf)
		
# Results. Around the same as fitting a ridge per feature but waay easier/faster
Layer size: 50
alpha =  10
mse = 0.0257794951244 mse3 = 0.0223201188953 corr = 0.475850131809 corr/f = 0.0529520142246
alpha =  21.5
mse = 0.0242249855181 mse3 = 0.0208086534726 corr = 0.506984272708 corr/f = 0.0600512462354
alpha =  46.3
mse = 0.0230861075072 mse3 = 0.0196558398647 corr = 0.533839972878 corr/f = 0.0667489643735
alpha =  100
mse = 0.0218639488688 mse3 = 0.0193035890575 corr = 0.564555705951 corr/f = 0.0682887109243
alpha =  215
mse = 0.0212770194296 mse3 = 0.0192422686163 corr = 0.572942119843 corr/f = 0.07834037493
alpha =  463
mse = 0.0214436296667 mse3 = 0.0193405502465 corr = 0.565277988664 corr/f = 0.00347691794447
alpha =  1000
mse = 0.0213355846686 mse3 = 0.0193231800619 corr = 0.567563383215 corr/f = 0.00613067976954
alpha =  2150
mse = 0.0213397212161 mse3 = 0.0193270911939 corr = 0.567462174522 corr/f = 0.0017997517905
alpha =  4630
mse = 0.0213398353438 mse3 = 0.0193273113344 corr = 0.567459801195 corr/f = 0.00464508241859
alpha =  10000
mse = 0.021339818236 mse3 = 0.0193273888153 corr = 0.56746023488 corr/f = 0.00612134107906
alpha =  21500
mse = 0.0213398201414 mse3 = 0.0193274589446 corr = 0.56746004767 corr/f = -0.00156667390074
Layer size: 100
alpha =  10
mse = 0.0258631621736 mse3 = 0.0227344928271 corr = 0.471930747042 corr/f = 0.0480801354604
alpha =  21.5
mse = 0.0252741449719 mse3 = 0.021152285919 corr = 0.489634513372 corr/f = 0.043305588632 # Not sure what happen here, if this point looks odd then recompute.
alpha =  46.3
mse = 0.0231419263243 mse3 = 0.0193468815388 corr = 0.533864854189 corr/f = 0.0608995757062
alpha =  100
mse = 0.0216490637053 mse3 = 0.018890554959 corr = 0.567559948874 corr/f = 0.0765636772797
alpha =  215
mse = 0.0211889650818 mse3 = 0.0193733712221 corr = 0.571786788095 corr/f = 0.0660530958462
alpha =  463
mse = 0.0214296611932 mse3 = 0.019352664085 corr = 0.565534615723 corr/f = 0.00491760980511
alpha =  1000
mse = 0.0213456723867 mse3 = 0.0193258733664 corr = 0.567357518334 corr/f = 0.013733524184
alpha =  2150
mse = 0.0213394322316 mse3 = 0.019328770254 corr = 0.567475894533 corr/f = -0.0040017622255
alpha =  4630
mse = 0.0213394858494 mse3 = 0.0193280082184 corr = 0.567476886 corr/f = 0.00287664537525
alpha =  10000
mse = 0.0213394667719 mse3 = 0.019327350433 corr = 0.567477735846 corr/f = 0.00175524885843
alpha =  21500
mse = 0.0213394448105 mse3 = 0.0193272032291 corr = 0.56747841095 corr/f = 0.00166323039771
Layer size: 300
alpha =  10
mse = 0.0260725378251 mse3 = 0.0228571570571 corr = 0.470441155721 corr/f = 0.0384161393257
alpha =  21.5
mse = 0.0251110313619 mse3 = 0.0211006520892 corr = 0.496196150552 corr/f = 0.0400270241815
alpha =  46.3
mse = 0.0232015322766 mse3 = 0.0194845099388 corr = 0.533432614286 corr/f = 0.0616957571677
alpha =  100
mse = 0.0219354229974 mse3 = 0.0192759896709 corr = 0.564954651682 corr/f = 0.0683304329853
alpha =  215
mse = 0.0212039626318 mse3 = 0.019342100062 corr = 0.57310391036 corr/f = 0.0802871382986
alpha =  463
mse = 0.0213705932058 mse3 = 0.0193239345303 corr = 0.566903191901 corr/f = 0.0104376740773
alpha =  1000
mse = 0.0213424901302 mse3 = 0.0193266136762 corr = 0.567386906087 corr/f = 0.0127275071293
alpha =  2150
mse = 0.0213328988273 mse3 = 0.0193034814474 corr = 0.567652606723 corr/f = 0.0072176873177
alpha =  4630
mse = 0.0213361177634 mse3 = 0.0193061046394 corr = 0.567572092199 corr/f = 0.000697572759059
alpha =  10000
mse = 0.0213370602796 mse3 = 0.019307164774 corr = 0.567549561938 corr/f = -0.00282093413539
alpha =  21500
mse = 0.0213374263114 mse3 = 0.0193076680987 corr = 0.56754092582 corr/f = -0.000853125875355
Layer size: 600
alpha =  10
mse = 0.0260887206105 mse3 = 0.0215400722144 corr = 0.471187788438 corr/f = 0.0399059834396
alpha =  21.5
mse = 0.0247148040095 mse3 = 0.0221236013114 corr = 0.498721047373 corr/f = 0.0495444465389
alpha =  46.3
mse = 0.0232557987956 mse3 = 0.0201565579994 corr = 0.534112591889 corr/f = 0.06215584304
alpha =  100
mse = 0.0219352254296 mse3 = 0.0199665756205 corr = 0.569983391721 corr/f = 0.0779436978553
alpha =  215
mse = 0.0212320864021 mse3 = 0.0194206775555 corr = 0.572862293871 corr/f = 0.0784011805555
alpha =  463
mse = 0.0213820923424 mse3 = 0.0193250634939 corr = 0.566509428758 corr/f = 0.0124097807367
alpha =  1000
mse = 0.0213415308323 mse3 = 0.0193262015748 corr = 0.567403155821 corr/f = 0.0171636298828
alpha =  2150
mse = 0.0213455453224 mse3 = 0.0193216153386 corr = 0.567329610688 corr/f = 0.00215253033149
alpha =  4630
mse = 0.0213370161142 mse3 = 0.0193090150245 corr = 0.567548647894 corr/f = -0.00175419291273
alpha =  10000
mse = 0.0213395374043 mse3 = 0.0193276619304 corr = 0.567465458904 corr/f = 0.00421691427993
alpha =  21500
mse = 0.0213395223843 mse3 = 0.0193268807089 corr = 0.567466448453 corr/f = 0.000253731896486
Layer size: 1000
alpha =  10
mse = 0.0265585833527 mse3 = 0.0230378402865 corr = 0.471990661908 corr/f = 0.0310937981763
alpha =  21.5
mse = 0.0249892793224 mse3 = 0.0225323133461 corr = 0.49236495663 corr/f = 0.0497230041675
alpha =  46.3
mse = 0.0235179005409 mse3 = 0.0205721973155 corr = 0.53182893376 corr/f = 0.0582233499612
alpha =  100
mse = 0.0220826922109 mse3 = 0.0204719708317 corr = 0.568199958608 corr/f = 0.0746761624304
alpha =  215
mse = 0.0211937234369 mse3 = 0.0194136479731 corr = 0.572410708259 corr/f = 0.0674437600081
alpha =  463
mse = 0.021377704465 mse3 = 0.0193348774486 corr = 0.566795891669 corr/f = 0.0110049776324
alpha =  1000
mse = 0.0213364260758 mse3 = 0.0193348376804 corr = 0.567564849236 corr/f = -0.00341983140096
alpha =  2150
mse = 0.0213401648489 mse3 = 0.0193204923822 corr = 0.567466095663 corr/f = -0.00427633714749
alpha =  4630
mse = 0.021331757624 mse3 = 0.019318327796 corr = 0.56768415739 corr/f = 0.00573835307107
alpha =  10000
mse = 0.0213346897015 mse3 = 0.0193142161331 corr = 0.567613266848 corr/f = -0.000437152774056
alpha =  21500
mse = 0.0213356661625 mse3 = 0.019310546446 corr = 0.567589071613 corr/f = -0.000470723273639
Layer size: 1500
alpha =  10
mse = 0.0261710037941 mse3 = 0.0221435929699 corr = 0.478628008497 corr/f = 0.0374274742501
alpha =  21.5
mse = 0.025362436466 mse3 = 0.0213998734551 corr = 0.498368365331 corr/f = 0.0406728035666
alpha =  46.3
mse = 0.0237691069191 mse3 = 0.0209187486333 corr = 0.528700049579 corr/f = 0.0562011832864
alpha =  100
mse = 0.021777175282 mse3 = 0.0197335973784 corr = 0.571901742482 corr/f = 0.0830747514617
alpha =  215
mse = 0.0212040915712 mse3 = 0.0195057068127 corr = 0.573143506854 corr/f = 0.0688530993488
alpha =  463
mse = 0.021337164306 mse3 = 0.0193401682495 corr = 0.567488772056 corr/f = 0.0195911854038
alpha =  1000
mse = 0.0213822068461 mse3 = 0.0193231488836 corr = 0.566837653255 corr/f = 0.00885475609409
alpha =  2150
mse = 0.0213406152518 mse3 = 0.0193184726228 corr = 0.567455827405 corr/f = -0.00690930570814
alpha =  4630
mse = 0.0213331387496 mse3 = 0.0193257845825 corr = 0.567640122872 corr/f = 0.00771921659787
alpha =  10000
mse = 0.0213346096265 mse3 = 0.0193151925181 corr = 0.567629360016 corr/f = -0.00107231469099
alpha =  21500
mse = 0.0213359216314 mse3 = 0.0193123518948 corr = 0.567599934545 corr/f = 0.00288799178625
Layer size: 2000
alpha =  10
mse = 0.026432061277 mse3 = 0.0220081146551 corr = 0.475715025432 corr/f = 0.0327682490817
alpha =  21.5
mse = 0.0252208670348 mse3 = 0.0241684256146 corr = 0.492265311105 corr/f = 0.0532512693596
alpha =  46.3
mse = 0.0239662303282 mse3 = 0.020253853335 corr = 0.527043110978 corr/f = 0.0531655050424
alpha =  100
mse = 0.0219310590395 mse3 = 0.0196766822472 corr = 0.570598865182 corr/f = 0.0753504516038
alpha =  215
mse = 0.0212345891837 mse3 = 0.0194837699112 corr = 0.573167131208 corr/f = 0.0677792074444
alpha =  463
mse = 0.021336924672 mse3 = 0.0193509673273 corr = 0.567507466933 corr/f = 0.0213079405048
alpha =  1000
mse = 0.0213276449261 mse3 = 0.0193436306615 corr = 0.567813678403 corr/f = -0.00176557449659
alpha =  2150
mse = 0.0213395900554 mse3 = 0.0193263463845 corr = 0.567508088252 corr/f = 0.000849154455796
alpha =  4630
mse = 0.0213448523614 mse3 = 0.0193154688661 corr = 0.567379234325 corr/f = -0.013370490629
alpha =  10000
mse = 0.0213334153005 mse3 = 0.0193146361223 corr = 0.567659437295 corr/f = 0.00047718043892
alpha =  21500
mse = 0.0213347973804 mse3 = 0.0193127918618 corr = 0.567627563092 corr/f = -0.00176512128907

# Plotting results
from mpl_toolkits.mplot3d import Axes3D

mse = np.array([[0.02578, 0.02422, 0.02309, 0.02186, 0.02128, 0.02144, 0.02134, 0.02134, 0.02134, 0.02134, 0.02134], [0.02586, 0.02527, 0.02314, 0.02165, 0.02119, 0.02143, 0.02135, 0.02134, 0.02134, 0.02134, 0.02134], [0.02607, 0.02511, 0.02320, 0.02194, 0.02120, 0.02137, 0.02134, 0.02133, 0.02134, 0.02134, 0.02134], [0.02608, 0.02471, 0.02326, 0.02194, 0.02123, 0.02138, 0.02134, 0.02135, 0.02134, 0.02134, 0.02134], [0.02656, 0.02499, 0.02351, 0.02208, 0.02119, 0.02138, 0.02134, 0.02134, 0.02133, 0.02133, 0.02134], [0.02617, 0.02536, 0.02377, 0.02178, 0.02120, 0.02134, 0.02138, 0.02134, 0.02133, 0.02133, 0.02134], [0.02643, 0.02522, 0.02396, 0.02193, 0.02124, 0.02134, 0.02133, 0.02134, 0.02134, 0.02133, 0.02133]])

xs, ys = np.meshgrid(regularization_params, hidden_layer_size)
fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_wireframe(np.log10(xs), ys, mse)
ax.set_xlabel('Log10(alpha)'); ax.set_ylabel('Hidden units'); ax.set_zlabel('MSE')

# Hidden layer size doesn't seem to matter so much as long as it is between 100-1000. Low layers seem more stable (close to ptimal result even with diff alphas)
# Better regularization term was always 215 (10^2.33)

# Searhcing whether 400 is better or worse than 200
regularization_params = 10 ** np.array([2, 2.2, 2.4, 2.6, 2.8, 3])
hidden_layer_size = [200, 400]

Layer size: 200
alpha =  100.0
mse = 0.0220333253456 mse3 = 0.0193833749654 corr = 0.563835795194 corr/f = 0.0644525553303
alpha =  158.489319246
mse = 0.0215248838887 mse3 = 0.0193002644536 corr = 0.571153584581 corr/f = 0.0743276674374
alpha =  251.188643151
mse = 0.0213196110593 mse3 = 0.0194337238678 corr = 0.569886747168 corr/f = 0.0665708024669
alpha =  398.107170553
mse = 0.0213319477386 mse3 = 0.0193475704975 corr = 0.56779248167 corr/f = 0.0194849958439
alpha =  630.95734448
mse = 0.0213425302574 mse3 = 0.0193304276071 corr = 0.567392294752 corr/f = 0.0204292299157
alpha =  1000.0
mse = 0.021345495831 mse3 = 0.019327212813 corr = 0.567336004014 corr/f = 0.0142517196875
Layer size: 400
alpha =  100.0
mse = 0.0217214514423 mse3 = 0.0191332419605 corr = 0.568800462709 corr/f = 0.0799652759934
alpha =  158.489319246
mse = 0.0211364783201 mse3 = 0.0191427142156 corr = 0.573893190122 corr/f = 0.0838578245505
alpha =  251.188643151
mse = 0.0213269499534 mse3 = 0.0194241674757 corr = 0.570079171374 corr/f = 0.0650127450523
alpha =  398.107170553
mse = 0.0213297570452 mse3 = 0.0193501747419 corr = 0.567800089072 corr/f = 0.0209545928471
alpha =  630.95734448
mse = 0.0213422368131 mse3 = 0.019327084102 corr = 0.567389070617 corr/f = 0.0220152728808
alpha =  1000.0
mse = 0.0213419011323 mse3 = 0.0193269836055 corr = 0.567411180712 corr/f = 0.0145263851509

# Strick with 400

# Test one network with two hidden layers (300->300)
regularization_params = [21.5, 46.3, 100, 215, 463, 1000, 2150, 4630, 10000]

alpha =  21.5
mse = 0.0228966996663 mse3 = 0.0196892463847 corr = 0.538560540501 corr/f = 0.0676796587479
alpha =  46.3
mse = 0.0220260386894 mse3 = 0.0195098401806 corr = 0.55774486202 corr/f = 0.0532677539696
alpha =  100
mse = 0.0214234003779 mse3 = 0.019263581127 corr = 0.567487576571 corr/f = 0.0280454641948
alpha =  215
mse = 0.0214523526927 mse3 = 0.0193270153001 corr = 0.564240993667 corr/f = 0.00159946791642
alpha =  463
mse = 0.0213819339502 mse3 = 0.0193399706604 corr = 0.566257509163 corr/f = -0.00117184619429
alpha =  1000
mse = 0.021352342929 mse3 = 0.0193126689912 corr = 0.567168860543 corr/f = -0.00958043061637
alpha =  2150
mse = 0.021339080185 mse3 = 0.019339352144 corr = 0.567480838063 corr/f = 0.00190309643806
alpha =  4630
mse = 0.0213398165077 mse3 = 0.0193378131581 corr = 0.567455277239 corr/f = -8.20879558356e-05
alpha =  10000
mse = 0.0213396112238 mse3 = 0.0193311283297 corr = 0.567462985749 corr/f = -0.000678624780432
# Worse results. Higher risk of overfitting.

# Test early stopping off (results may be better just because it has more data,
# anyways those are good news)
regularization_params = 10 ** np.array([2, 2.2, 2.4, 2.6, 2.8, 3])
layer_size = 400

alpha =  100.0
mse = 0.021822115954 mse3 = 0.0190615147663 corr = 0.555998107224 corr/f = 0.0744424501327
alpha =  158.489319246
mse = 0.0209844592553 mse3 = 0.0191216088448 corr = 0.577364254321 corr/f = 0.09561386117
alpha =  251.188643151
mse = 0.0209760471482 mse3 = 0.0191741523787 corr = 0.577398076758 corr/f = 0.0977662880596
alpha =  398.107170553
se = 0.0211942698959 mse3 = 0.0193383342083 corr = 0.571679075825 corr/f = 0.061776641471
alpha =  630.95734448
mse = 0.0213163003053 mse3 = 0.0193133423227 corr = 0.568139150887 corr/f = 0.0250667488862
alpha =  1000.0
mse = 0.0213322072951 mse3 = 0.0193074390209 corr = 0.567667220087 corr/f = 0.0180124172072


# Test SGD+ NAG + adaptive larning rate (early stoping off). This is pretty much the
# best I coudl do.
regularization_params = 10 ** np.array([2, 2.2, 2.4, 2.6, 2.8, 3])
layer_size = 400

alpha =  100.0
mse = 0.0215476880499 mse3 = 0.0190913976281 corr = 0.565911366868 corr/f = 0.0834153861356
alpha =  158.489319246
mse = 0.0210374974137 mse3 = 0.0189381169213 corr = 0.575219219726 corr/f = 0.0959284577956
alpha =  251.188643151
mse = 0.0209549012738 mse3 = 0.0191269740624 corr = 0.577713754425 corr/f = 0.102252712003
alpha =  398.107170553
mse = 0.0211683240322 mse3 = 0.0193243680884 corr = 0.572235791085 corr/f = 0.0783581690361
alpha =  630.95734448
mse = 0.0213151360533 mse3 = 0.0193325706545 corr = 0.568144921976 corr/f = 0.0269850428148
alpha =  1000.0
mse = 0.0213349746994 mse3 = 0.0193254273956 corr = 0.567574260392 corr/f = 0.0134085680914
# Takes more time and results are slightly better than ADAM. Time is still waaaay less tha fitting a ridge inm a single feature. Whole model takes 10 min!!

# Feature selection and then neural network. Deleting the noisy features may
# improve the answer. I do feature selection based on asingle y_i (y_train[:,50]).
# If it seems to work I could average over all features.
num_features = [100, 500, 1000, 3000]
regularization_params = 10 ** np.array([0, 0.333, 0.666, 1, 1.333, 1.666, 2, 2.333, 2.6666, 3])
layer_size = 400

for k in num_features:
	print(k, 'feature(s)'); selector = SelectKBest(f_regression, k=k).fit(X_train, y_train[:,50]); X_new = selector.transform(X_train); X_test_new = selector.transform(X_test);
	
	for alpha in regularization_params:
		print('alpha = ', alpha); model = MLPRegressor(hidden_layer_sizes = (layer_size,), alpha=alpha, max_iter=1000, random_state=123, algorithm='sgd', learning_rate='adaptive', tol = 1e-8); model.fit(X_new, y_train); y_pred = model.predict(X_test_new); mse = ((y_pred-y_test)**2).mean(); mse3 = ((y_pred[:,3] - y_test[:,3])**2).mean(); corr = 0; corrf = 0;
		for i in range(718):
			corr +=  np.corrcoef(y_pred[i,:], y_test[i,:])[0,1]
		for i in range(768):
			corrf +=  np.corrcoef(y_pred[:,i], y_test[:,i])[0,1]
		corr =	corr/718; corrf =	corrf/768; print('mse =', mse, 'mse3 =',mse3, 'corr =', corr, 'corr/f =', corrf)


100 feature(s)
alpha =  1.0
mse = 0.0238635681155 mse3 = 0.0211029239276 corr = 0.515343390954 corr/f = 0.0302434466513
alpha =  2.15278173472
mse = 0.0229672500409 mse3 = 0.0204554942258 corr = 0.531425712043 corr/f = 0.0340229228823
alpha =  4.63446919736
mse = 0.02187657334 mse3 = 0.019915469168 corr = 0.554957399611 corr/f = 0.055659758654
alpha =  10.0
mse = 0.0212054076639 mse3 = 0.0196978017393 corr = 0.571242335417 corr/f = 0.0822580021609
alpha =  21.5278173472
mse = 0.020987382678 mse3 = 0.01939522832 corr = 0.577132167239 corr/f = 0.095425022481
alpha =  46.3446919736
mse = 0.0210061027883 mse3 = 0.0193982338385 corr = 0.57679432947 corr/f = 0.089404991498
alpha =  100.0
mse = 0.0211230946148 mse3 = 0.0193956489477 corr = 0.573653745809 corr/f = 0.0701366685823
alpha =  215.278173472
mse = 0.0212822414609 mse3 = 0.0193444724006 corr = 0.56911675109 corr/f = 0.0521875790608
alpha =  464.087637808
mse = 0.0213367466246 mse3 = 0.0193251327115 corr = 0.567528080307 corr/f = 0.00368565660554
alpha =  1000.0
mse = 0.0213368340155 mse3 = 0.0193248170323 corr = 0.567523988082 corr/f = -0.00739587040845
500 feature(s)
alpha =  1.0
mse = 0.0255866380636 mse3 = 0.0232182054739 corr = 0.494160305014 corr/f = 0.0412720263862
alpha =  2.15278173472
mse = 0.0248204354967 mse3 = 0.0225147085293 corr = 0.504585476056 corr/f = 0.0465341260556
alpha =  4.63446919736
mse = 0.0234967982724 mse3 = 0.0210033619466 corr = 0.527276776695 corr/f = 0.0568947583711
alpha =  10.0
mse = 0.0222265761063 mse3 = 0.0199361468809 corr = 0.552176410877 corr/f = 0.0719176881352
alpha =  21.5278173472
mse = 0.0212732100993 mse3 = 0.0193809868808 corr = 0.571396216777 corr/f = 0.0925552073082
alpha =  46.3446919736
mse = 0.0208711029825 mse3 = 0.019346212311 corr = 0.580398962613 corr/f = 0.110550291122
alpha =  100.0
mse = 0.0209390260442 mse3 = 0.019399128318 corr = 0.57859870544 corr/f = 0.101947609801
alpha =  215.278173472
mse = 0.0211798465998 mse3 = 0.019384673722 corr = 0.572021165101 corr/f = 0.0639714002599
alpha =  464.087637808
mse = 0.0213320455703 mse3 = 0.019322874426 corr = 0.567667305213 corr/f = 0.0420130047454
alpha =  1000.0
mse = 0.0213364524764 mse3 = 0.0193239627772 corr = 0.56753513126 corr/f = 0.0169784725185
1000 feature(s)
alpha =  1.0
mse = 0.0278664552344 mse3 = 0.0253004959506 corr = 0.464416605331 corr/f = 0.0402574463068
alpha =  2.15278173472
mse = 0.0269369661471 mse3 = 0.0240489245782 corr = 0.470538204037 corr/f = 0.0440719462651
alpha =  4.63446919736
mse = 0.0249743714064 mse3 = 0.0223306447997 corr = 0.496952485205 corr/f = 0.0478986531606
alpha =  10.0
mse = 0.0232412244016 mse3 = 0.0211291213147 corr = 0.530872281107 corr/f = 0.0587219456359
alpha =  21.5278173472
mse = 0.0220210505992 mse3 = 0.020012704385 corr = 0.55522391602 corr/f = 0.0719138262376
alpha =  46.3446919736
mse = 0.021133408026 mse3 = 0.0194762447793 corr = 0.573492413235 corr/f = 0.0946342305035
alpha =  100.0
mse = 0.0209017642219 mse3 = 0.0193440772411 corr = 0.57931479843 corr/f = 0.107159128498
alpha =  215.278173472
mse = 0.0211115856841 mse3 = 0.0193780576628 corr = 0.573832753837 corr/f = 0.0814455338272
alpha =  464.087637808
mse = 0.0213083242315 mse3 = 0.0193341253205 corr = 0.568349899141 corr/f = 0.0402951337189
alpha =  1000.0
mse = 0.0213364794915 mse3 = 0.0193235888316 corr = 0.567535050885 corr/f = 0.0213979766597
3000 feature(s)
alpha =  1.0
mse = 0.0309779230158 mse3 = 0.0286391546492 corr = 0.418335041438 corr/f = 0.0256535777843 # Did not converge
alpha =  2.15278173472
mse = 0.0287801127092 mse3 = 0.0263343890718 corr = 0.442268039188 corr/f = 0.0294566096004 # Did not converge
alpha =  4.63446919736
mse = 0.0270129854278 mse3 = 0.0249716202641 corr = 0.465324312904 corr/f = 0.0328469759155 # Did not converge
alpha =  10.0
mse = 0.025377710219 mse3 = 0.0227543517814 corr = 0.490022020416 corr/f = 0.0369579013578
alpha =  21.5278173472
mse = 0.0234733297132 mse3 = 0.02109854676 corr = 0.524038022014 corr/f = 0.0479643455767
alpha =  46.3446919736
mse = 0.0218200083817 mse3 = 0.0196982282472 corr = 0.559232370992 corr/f = 0.0763474131438
alpha =  100.0
mse = 0.0210077013632 mse3 = 0.0191492179781 corr = 0.576184552134 corr/f = 0.0993754802087
alpha =  215.278173472
mse = 0.0210021326236 mse3 = 0.0193448766799 corr = 0.576724927516 corr/f = 0.100737138746
alpha =  464.087637808
mse = 0.021288619783 mse3 = 0.0193372381131 corr = 0.568890367544 corr/f = 0.0475782270684
alpha =  1000.0
mse = 0.0213370311191 mse3 = 0.0193240176896 corr = 0.567520347669 corr/f = 0.0111564084382

# Slightly better for a 1000-500, seems hard to explain, though.

# Trains fast (less than an hour) and results are average, could have it 
# as a baseline
